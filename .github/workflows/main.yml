name: Daily Web Scraping

on:
  schedule:
    - cron: '0 3 * * *'  # Runs every day at midnight UTC
  workflow_dispatch:  # Allows manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      # Check out the repository code
      - name: Checkout code
        uses: actions/checkout@v4

      # Set up Python
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'  # Adjust if you need a different version

      # Install Chrome and ChromeDriver
      - name: Install Chrome and ChromeDriver
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          CHROME_VERSION=$(google-chrome --version | grep -oP '\d+\.\d+\.\d+')
          CHROMEDRIVER_VERSION=$(curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE_$CHROME_VERSION)
          wget -O /tmp/chromedriver.zip "https://chromedriver.storage.googleapis.com/$CHROMEDRIVER_VERSION/chromedriver_linux64.zip"
          sudo unzip /tmp/chromedriver.zip -d /usr/local/bin/
          sudo chmod +x /usr/local/bin/chromedriver

      # Install Python dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 firebase-admin selenium

      # Set up Firebase credentials
      - name: Set up Firebase credentials
        env:
          FIREBASE_CREDENTIALS: ${{ secrets.FIREBASE_CREDENTIALS }}
        run: |
          echo "$FIREBASE_CREDENTIALS" > firebase_credentials.json

      # Run the scraping script
      - name: Run scraping script
        run: python main.py

      # Optional: Clean up credentials file
      - name: Clean up
        if: always()
        run: rm -f firebase_credentials.json
